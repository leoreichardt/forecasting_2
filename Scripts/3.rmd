---
title: "Untitled"
output: html_document
---
```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(readr)
library(tidyverse)
library(ggseas)
library(hts)
library(dplyr)
library(stringr)
library(purrr)
library(tidyr)
library(here)
library(stringi) 
library(kableExtra)
```

# Modeling

We are dealing with high frequency data (daily) which can be a problem when fitting some model. Therefore, the easiest solution is to set the frequency of the time series to 7.  

To avoid any problem related to the frequency used when creating the time series, we decided to use brackets in order to create a training and validation set.

According to the paper *Another look at measures of forecast accuracy* (R. Hyndman, A. Koehler), using the MASE would ensure us easily applicable metric among our methods.  

Moreover, being the mean absolute error of the forecast values, divided by the mean absolute error of the in-sample one-step naive forecast, it allows us to use it as a benchmark.
Thus, a MASE with a value higher than one would mean that our model is less attractive than in-sample one-step forecasts from the naive method and thus should not be considered.

This metric is easy to obtain when we use the forecast function however, in the second part we will use auto.arima which does not automatically return the MASE.
Thus, this second part will be analyzed separately using the RMSE.

## Hierarchical modeling

Having to analyze the TX_1 store, we started by trying to modelize the sales using the underlying levels.  
We were soon confronted with the problem of the lack of power of our laptops. In fact, the bottom level accounts 3049 products which is too heavy.  
We decided to keep only the levels above : store_id (= TX_1), cat_id and dept_id.
In the ARIMA models, we included regressors to take into account the effect of Christmas and Independance day. This technique improved the metrics.  

```{r load data, message=FALSE, warning=FALSE, include=FALSE}
# Load data

df <- read_csv("~/Documents/GitHub/forecasting_2/Data/sales_train_validation.csv")
df <- data.frame(df, stringsAsFactors = FALSE)
df <- within(df, 
             item_id <- as.factor(item_id),           
             dept_id <- as.factor(dept_id),
             cat_id <- as.factor(cat_id))

cal <- read.csv("~/Documents/GitHub/forecasting_2/Data/calendar.csv", na.strings = "")
prices <- read_csv("~/Documents/GitHub/forecasting_2/Data/sell_prices.csv")
prices <- data.frame(prices, stringsAsFactors = FALSE)
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
tdf <- t(df[,7:ncol(df)])

tdf2 <- rbind(tdf, 
              matrix(NA, nrow = nrow(cal) - nrow(tdf),
                     ncol = ncol(tdf)))
tdf3 <- data.frame(cbind(cal, tdf2))

colnames(tdf3) <- c(colnames(tdf3)[1:14], df$id)


tdf3$Christmas <- ifelse(tdf3$event_name_1 == "Christmas", 1,0)
tdf3$Christmas[is.na(tdf3$Christmas)] = 0

tdf3$IndependenceDay <- ifelse(tdf3$event_name_1 == "IndependenceDay", 1,0)
tdf3$IndependenceDay[is.na(tdf3$IndependenceDay)] = 0



specialDays_modeling <- cbind(tdf3$IndependenceDay[1:1857], tdf3$Christmas[1:1857])
specialDays_forecasting <- cbind(tdf3$IndependenceDay[1858:1885], tdf3$Christmas[1858:1885])
```



```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# create the data frame
df_TX1 <- subset(df, store_id == "TX_1")

tdf_TX1 <- t(df_TX1[,7:ncol(df_TX1)])

tdf2_TX1 <- rbind(tdf_TX1, 
              matrix(NA, nrow = nrow(cal) - nrow(tdf_TX1),
                     ncol = ncol(tdf_TX1)))
tdf3_TX1 <- data.frame(cbind(cal, tdf2_TX1))

colnames(tdf3_TX1) <- c(colnames(tdf3_TX1)[1:14], df_TX1)
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# create time series training / validation test
TS_TX1 <- ts(tdf3_TX1[,-(1:14)], frequency = 7)
TS_TX1_tr <- TS_TX1[1:1857,]
TS_TX1_val <- TS_TX1[1858:1885,]
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# which series are nested in another.
splitted <- do.call(rbind, stri_split_fixed(df_TX1$id, '_'))[,1:5]
splitted[,1] <- substr(splitted[,1], 1, 3)
hts_ids <- apply(splitted, 1, paste0, collapse = '')
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# create hts objects
HTS_TX1_tr <- hts(TS_TX1_tr, bnames = hts_ids, characters = c(3,1,6))
HTS_TX1_val <- hts(TS_TX1_val, bnames = hts_ids, characters = c(3,1,6))
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# do not analyse article level
TS_TX1_tr <- HTS_TX1_tr %>% aggts(levels=0:2) # do not take into consideration level 3
TS_TX1_val <- HTS_TX1_val %>% aggts(levels=0:2)

#put back in hts object
hts_ids_lvl_0_to_2 <- str_sub(hts_ids, end = 4) %>% unique() # get the bname without articles


HTS_TX1_tr <- hts(TS_TX1_tr[, -(1:4)], bnames = hts_ids_lvl_0_to_2, characters = c(3,1))
HTS_TX1_val <- hts(TS_TX1_val[, -(1:4)], bnames = hts_ids_lvl_0_to_2, characters = c(3,1))
```
**Top level : store_id (= TX_1)**  
**Bottom level : dept_id**  

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
FORECAST_TX1_bu_arima <- forecast(HTS_TX1_tr, h = 28, method = "bu", fmethod = "arima", xreg = specialDays_modeling, newxreg = specialDays_forecasting)
ACC_TX1_bu_arima <- accuracy.gts(FORECAST_TX1_bu_arima, HTS_TX1_val) 

FORECAST_TX1_bu_ets <- forecast(HTS_TX1_tr,h =28, method = "bu", fmethod = "ets")
ACC_TX1_bu_ets <- accuracy.gts(FORECAST_TX1_bu_ets, HTS_TX1_val) 

FORECAST_TX1_tdfp_arima <- forecast(HTS_TX1_tr,h =28, method = "tdfp", fmethod = "arima", xreg = specialDays_modeling, newxreg = specialDays_forecasting)
ACC_TX1_tdfp_arima <- accuracy.gts(FORECAST_TX1_tdfp_arima, HTS_TX1_val)  

FORECAST_TX1_tdfp_ets <- forecast(HTS_TX1_tr,h =28, method = "tdfp", fmethod = "ets")
ACC_TX1_tdfp_ets <- accuracy.gts(FORECAST_TX1_tdfp_ets, HTS_TX1_val)  

FORECAST_TX1_comb <- forecast(HTS_TX1_tr,h =28, method = "comb", weights = "mint", covariance = "shr")
ACC_TX1_bu_comb <- accuracy.gts(FORECAST_TX1_comb, HTS_TX1_val)  

ACC <- data.frame(ACC_TX1_bu_arima, ACC_TX1_bu_ets, ACC_TX1_tdfp_arima, ACC_TX1_tdfp_ets, ACC_TX1_bu_comb)
ACC <- round(ACC, digits = 3)

kableExtra::kable(ACC[, c(1,12,23,34,45)], col.names = c("ARIMA", "ETS", "ARIMA", "ETS", "mint - shr"),
    caption = "<center><strong> 28 days forcecast : metrics of different models</strong></center>",
    escape = FALSE,
    format = "html") %>%
    kableExtra::kable_styling(
        bootstrap_options = c("striped","bordered"),
        full_width = F) %>%
  add_header_above(c(" ", "Bottom-up" = 2, "Top-down" = 2, "Comb" = 1)) %>%
  column_spec(1, bold = T) %>%
  column_spec(2:6, width = "8em") %>%
row_spec(6, background = "lightyellow")


```

The best model is an ARIMA using a top-down approach.  

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
x <- FORECAST_TX1_tdfp_arima %>% aggts(levels = 0)
y <- HTS_TX1_val %>% aggts(levels = 0)
x <- data.frame(x, y, 1:28)
colnames(x) <- c("prediction", "actual", "time")

kable(x[,1:2], caption = "<center><strong>28 days forcecast</strong></center>",
    col.names = c("Predictions", "Observations"),
    escape = FALSE,
    format = "html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)

ggplot() + geom_line(x, mapping = aes(x = time, y = prediction), color = "red") + geom_line(x, mapping = aes(x = time, y = actual)) + ylab("Sales") + xlab("Time") + ggtitle("Observations and predictions", subtitle = "ARIMA model with bottom-up approach") + scale_y_continuous(limits = c(2000,4500)) + scale_x_continuous(breaks = seq(0,28,2)) + theme_bw()



```

Predictions are close to observed values.  
Other less conclusive trials can be found in the appendix. 




