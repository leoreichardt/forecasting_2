---
title: "Modeling at level_0 with ARIMA"
author: "Raffaello Raffin"
date: "5/20/2020"
output: html_document
---

```{r include=FALSE}
#set some options for the chunks
knitr::opts_chunk$set(
  fig.align = 'center'
)
```

```{r libraries, include=FALSE}
library(readr)
library(hts)
library(stringi)
library(dplyr)
library(stringr)
library(purrr)
library(tidyr)
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(kableExtra)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Load data
df <- read_csv("~/Documents/GitHub/forecasting_2/Data/sales_train_validation.csv")
df <- data.frame(df, stringsAsFactors = FALSE)
df <- within(df, 
             item_id <- as.factor(item_id),
             dept_id <- as.factor(dept_id),
             cat_id <- as.factor(cat_id))
cal <- read.csv("~/Documents/GitHub/forecasting_2/Data/calendar.csv", na.strings = "")
prices <- read_csv("~/Documents/GitHub/forecasting_2/Data/sell_prices.csv")
prices <- data.frame(prices, stringsAsFactors = FALSE)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Subset to a specific store
df <- subset(df, store_id == "TX_1")
 
# Take only the sales columns and transpose them
tdf <- t(df[,7:ncol(df)])

# Add "NA" values for the validation days;
# Could also drop the extra columns from from `cal`
tdf2 <- rbind(tdf, 
              matrix(NA, nrow = nrow(cal) - nrow(tdf),
                     ncol = ncol(tdf)))
tdf3 <- data.frame(cbind(cal, tdf2))

# Add in the unique names for each column
colnames(tdf3) <- c(colnames(tdf3)[1:14], df$id)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Add price information: 
# 1) Reshape the price data to have time series in rows, not columns
# 2) Since the prices are given per week, repeat them accordingly to 
#    get a full time series
# 3) For those prices which are absent at the beginning of the data,
#    we fill the missing values with the first observed price.

# 1)
prices <- subset(prices, store_id == "TX_1") %>% select(-store_id,) %>%
  spread(item_id, sell_price)
colnames(prices) <- c("wm_yr_wk", paste0("PRICE_", colnames(prices)[-1]))

# 2)
repetitions <- table(tdf3$wm_yr_wk)
prices_full <- data.frame(wm_yr_wk = rep(prices$wm_yr_wk, times = repetitions))
for (j in 2:ncol(prices))
  prices_full <- cbind(prices_full, rep(prices[,j], times = repetitions))
colnames(prices_full) <- colnames(prices)

# 3)
are_na_at_beginning <- is.na(prices_full[1,])
for (j in which(are_na_at_beginning)) {
  first_non_na <- which(!is.na(prices_full[,j]))[1]
  prices_full[1:first_non_na,j] <- prices_full[first_non_na, j]
}
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Finally, combine all the data (volume + calendar + prices)
tdf4 <- merge(tdf3, prices_full, by = "wm_yr_wk")
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# In the following, we discard the price part and work only with the volume data.

# When using `hts`, we first need to create a multivariate time series object;
# for this we discard several calendar columns which are redundant.
TX1 <- ts(tdf3[,-(1:14)], frequency = 7)
training_set <- TX1[1:1857,]   ## on a 1913 jours avec data --> enlève 2x 28 (test/validation set)
validation_set <- TX1[1858:1885,]  ## on prend les premiers 28 jours qui serviront de test
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Next we have to create a way of denoting which series is nested in another.
# This is done by supplying the `bnames` argument to hts(), and here 
# we create the corresponding names by truncating the category names to 3 letters.

# 1) Separate out the parts in the id column
splitted <- do.call(rbind, stri_split_fixed(df$id, '_'))[,1:5]
# 2) Normalise the category names
splitted[,1] <- substr(splitted[,1], 1, 3)
hts_ids <- apply(splitted, 1, paste0, collapse = '')
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Create the hts object by specifying which parts of the series name correspond
# to which level of the hierarchy
ts_all <- hts(TX1, bnames = hts_ids, characters = c(3,1,6))
training_set <- hts(training_set, bnames = hts_ids, characters = c(3,1,6))
validation_set <- hts(validation_set, bnames = hts_ids, characters = c(3,1,6))
```


## Modeling at level_0 with ARIMA

We are dealing with high frequency data (daily) which can be a problem when fitting some model. Therefore, the easiest solution is to set the frequency of the time series to 7. 

However, we were able to notice in the Exploratory Data Analysis that our data presented several seasonalities. Thus, we have made several attempts to take them all into account. For example, we made the differentiations (single and double) “manually” before fiting a model. However, we were unable to undiff the forecasted data to get the true values. You can find these tests in the appendix.

In the next section we will work only on the top level (TX1).

```{r message=FALSE, warning=FALSE, include=FALSE}
#take only level 0
train_set <- training_set %>% aggts(levels=0)
val_set <- validation_set %>% aggts(levels=0)
```


### Auto.arima without prior transformation

To start, we simply apply the auto.arima function to make an Arima automatically without any prior transformation. The approximation and stepwise parameters are set to “FALSE”. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
#auto.arima sans tranformation préalable + forecast
model_auto <- train_set %>% auto.arima(approximation = FALSE, stepwise = FALSE)
checkresiduals(model_auto)
```
The result is an ARIMA(5,1,0). Residues seems to be normally distributed and centered around zero but we can see that there is still some dependency.

Indeed, on the top graph, we can clearly see the pattern linked to the closing of the store at Christmas. On the ACF, on the other hand, we note that there is still a weekly seasonality. These remarks are confirmed by the result of the Ljung-box test which gives us a value less than 0.05. Therefore, we can’t reject the null hypothesis that the time series isn’t autocorrelated.

We then carry out a forecast for the next 28 days and calculate the accuracy using the validation set (which corresponds to the same period). We get those metrics:

```{r echo=FALSE, message=FALSE, warning=FALSE}
#forecast
fc <- model_auto %>% forecast(h=28)
fc <- data.frame(fc)
fc <- ts(fc)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Metrics
acc <- accuracy(fc, val_set)
kableExtra::kable(acc,
    caption = "<center><strong>28 days forcecast accuracy metrics</strong></center>",
    escape = FALSE,
    format = "html")%>%
    kableExtra::kable_styling(
        bootstrap_options = c("striped","bordered"),
        full_width = F)
```

We find a RMSE of 499.9. In tests where we made the differentiations “manually”, we get lower values which is a sign of a better model. However, we made the choice not to display them here (but in the Appendix) because since we were not able to undiff the forecasted values, displaying them would not be useful for comparison.

Below, you can find the table and the corresponding plot with the predicted values as well as the confidence interval.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#table
valsettable <- val_set %>% as_data_frame()
fctable <- fc %>% as_data_frame() 
newtable <- cbind(valsettable, fctable) %>% select(-c(3:4)) %>% rename("Observations" = "Total")

newtable %>%
  kable(caption = "<center><strong>28 days forcecast</strong></center>",
    escape = FALSE,
    format = "html") %>% 
  kable_styling(bootstrap_options = c("striped","bordered","hover"), full_width = F) %>% 
  scroll_box(height = "400px")
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
#plot
 autoplot(val_set) +
  autolayer(fc, lty=2) +
   theme_bw() +
   labs(title = "Observations & Predictions",
        subtitle = "With confidence interval",
        y = "Sales") +
       theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5))
```
