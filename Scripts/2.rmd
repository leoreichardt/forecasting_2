---
title: "EDA time series"
author: "Raffaello Raffin"
date: "5/20/2020"
output: html_document
---

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(hts)
library(stringi)
library(dplyr)
library(stringr)
library(purrr)
library(tidyr)
library(lubridate)
library(knitr)
library(ggplot2)
library(kableExtra)
library(timeSeries)

```


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Load data


df <- read.csv("~/Documents/GitHub/forecasting_2/Data/sales_train_validation.csv")
df <- data.frame(df, stringsAsFactors = FALSE)
df <- within(df, 
             item_id <- as.factor(item_id),
             dept_id <- as.factor(dept_id),
             cat_id <- as.factor(cat_id))
cal <- read.csv("~/Documents/GitHub/forecasting_2/Data/calendar.csv", na.strings = "")
prices <- read.csv("~/Documents/GitHub/forecasting_2/Data/sell_prices.csv")
prices <- data.frame(prices, stringsAsFactors = FALSE)
```


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Subset to a specific store
df <- subset(df, store_id == "TX_1")
 
# Take only the sales columns and transpose them
tdf <- t(df[,7:ncol(df)])

# Add "NA" values for the validation days;
# Could also drop the extra columns from from `cal`
tdf2 <- rbind(tdf, 
              matrix(NA, nrow = nrow(cal) - nrow(tdf),
                     ncol = ncol(tdf)))
tdf3 <- data.frame(cbind(cal, tdf2))

# Add in the unique names for each column
colnames(tdf3) <- c(colnames(tdf3)[1:14], df$id)
```


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Add price information: 
# 1) Reshape the price data to have time series in rows, not columns
# 2) Since the prices are given per week, repeat them accordingly to 
#    get a full time series
# 3) For those prices which are absent at the beginning of the data,
#    we fill the missing values with the first observed price.

# 1)
prices <- subset(prices, store_id == "TX_1") %>% select(-store_id,) %>%
  spread(item_id, sell_price)
colnames(prices) <- c("wm_yr_wk", paste0("PRICE_", colnames(prices)[-1]))

# 2)
repetitions <- table(tdf3$wm_yr_wk)
prices_full <- data.frame(wm_yr_wk = rep(prices$wm_yr_wk, times = repetitions))
for (j in 2:ncol(prices))
  prices_full <- cbind(prices_full, rep(prices[,j], times = repetitions))
colnames(prices_full) <- colnames(prices)

# 3)
are_na_at_beginning <- is.na(prices_full[1,])
for (j in which(are_na_at_beginning)) {
  first_non_na <- which(!is.na(prices_full[,j]))[1]
  prices_full[1:first_non_na,j] <- prices_full[first_non_na, j]
}
```


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Finally, combine all the data (volume + calendar + prices)
tdf4 <- merge(tdf3, prices_full, by = "wm_yr_wk")
```


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# In the following, we discard the price part and work only with the volume data.

# When using `hts`, we first need to create a multivariate time series object;
# for this we discard several calendar columns which are redundant.
TX1 <- ts(tdf3[,-(1:14)], frequency = 7)
training_set <- TX1[1:1857,]   ## on a 1913 jours avec data --> enlève 2x 28 (test/validation set)
validation_set <- TX1[1858:1885,]  ## on prend les premiers 28 jours qui serviront de test
```


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Next we have to create a way of denoting which series is nested in another.
# This is done by supplying the `bnames` argument to hts(), and here 
# we create the corresponding names by truncating the category names to 3 letters.

# 1) Separate out the parts in the id column
splitted <- do.call(rbind, stri_split_fixed(df$id, '_'))[,1:5]
# 2) Normalise the category names
splitted[,1] <- substr(splitted[,1], 1, 3)
hts_ids <- apply(splitted, 1, paste0, collapse = '')
```


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Create the hts object by specifying which parts of the series name correspond
# to which level of the hierarchy
ts_all <- hts(TX1, bnames = hts_ids, characters = c(3,1,6))
training_set <- hts(training_set, bnames = hts_ids, characters = c(3,1,6))
validation_set <- hts(validation_set, bnames = hts_ids, characters = c(3,1,6))
```

## Time series anaylsis

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
training_set %>% aggts(level = 0) %>% 
  autoplot() +
  theme_bw() +
  labs(title = "TX1",
       subtitle = "Daily data",
       x = "Time",
       y = "Sales (#)") +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5))
```

By looking at daily sales, there seems to be an annual seasonality and a growing trend.
Moreover, the store is closed for Christmas every year. 

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
training_set %>% aggts(levels=1) %>%
  autoplot(facet=TRUE) +
   theme_bw() +
  labs(title = "Categories",
       subtitle = "Daily data",
       x = "Time",
       y = "Sales (#)") +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) +
  theme(strip.background = element_rect(fill="#69b3a2"))
```

If we decompose the time series by category, the seasonality we observed seems to come from FOOD while the trend is more towards hobbies and household.  
The first part of the household time series is marked by positive trend. This is probably due to the introduction of new products which creates a bias in the interpretation of the trend.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#we have high frequency data (daily)
#mstl: Decompose a time series into seasonal, trend and remainder components.

ts_365 <- ts(tdf3[1:1885,-(1:14)], frequency = 365)
hts_365 <- hts(ts_365, bnames = hts_ids, characters = c(3,1,6))

hts_365 %>% aggts(levels=0) %>%
  mstl() %>% 
  autoplot() +
  theme_bw() +
  labs(title = "Decomposition",
       subtitle = "Store TX1") +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) 
```
The trend is clear, however, the seasonality is not yet fully decomposed. As we have daily data, we probably have several seasonalities. In what follows, we try to decompose it manually


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
training_set %>% aggts(levels=0) %>% 
  ggAcf(lag.max = 1095) +
  theme_bw() +
  labs(title = "Correlogram",
       subtitle = "Annual seasonality") +
      theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) 

#The slow decrease in the ACF as the lags increase is due to the trend, while the “scalloped” shape is due the seasonality.
```

This correlogram shows 3 years. There are 3 waves with a peak approximately every 320 days which suggests a possible annual seasonality.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
training_set %>% aggts(levels=0) %>% 
  ggAcf(lag.max = 180) +
  theme_bw() +
  labs(title = "Correlogram",
       subtitle = "Monthly seasonality") +
      theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) 

#The slow decrease in the ACF as the lags increase is due to the trend, while the “scalloped” shape is due the seasonality.
```

This correlogram shows 180 lags. We can identify peaks every 30 days approximatly, which means that we have a monthly seasonality. 

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
training_set %>% aggts(levels=0) %>% 
  ggAcf(lag.max = 30) +
  theme_bw() +
  labs(title = "Correlogram",
       subtitle = "Weekly seasonality") +
      theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) 
```

This correlogram shows 30 lags. There is a peak every 7 days showing a weekly seasonality. 

## Conclusion of EDA

This EDA helps us to familiarize ourselves with the general data and more specifically with our TX_1 store. As far as sales are concerned, it seems to be a growing trend, even if it is difficult to analyse it because the number of products sold are not constant. 
The weekly and monthly seasonality are clear. The problem with a monthly seasonality is its lack of consistency, as not all months have the same number of days. Using a frequency of 30 or 31 would create a bias. Thus, we will not take it into account in our models. 

Concerning the annual seasonality, it seems implausible that every day of the year is correlated with the day of the previous year.   
However, as shown by the closing of stores at Christmas, we believe most of the correlation between years comes from special events that are repeated.  
Therefore, we will include in our models Christmas who has a big impact on sales (in fact, the stores are closed) and the only event that has taken place during the 28 days that we predict which is Independence Day.

