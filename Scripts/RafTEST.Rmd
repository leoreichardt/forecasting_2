---
title: "RafTEST"
author: "Raffaello Raffin"
date: "4/28/2020"
output: html_document
---

```{r libraries, include=FALSE}
library(readr)
library(hts)
library(stringi)
library(dplyr)
library(stringr)
library(purrr)
library(tidyr)
```


```{r}
# Load data
df <- read_csv("~/Documents/GitHub/forecasting_2/Data/sales_train_validation.csv")
df <- data.frame(df, stringsAsFactors = FALSE)
df <- within(df, 
             item_id <- as.factor(item_id),
             dept_id <- as.factor(dept_id),
             cat_id <- as.factor(cat_id))
cal <- read.csv("~/Documents/GitHub/forecasting_2/Data/calendar.csv", na.strings = "")
prices <- read_csv("~/Documents/GitHub/forecasting_2/Data/sell_prices.csv")
prices <- data.frame(prices, stringsAsFactors = FALSE)
```


```{r}
# Subset to a specific store
df <- subset(df, store_id == "TX_1")
 
# Take only the sales columns and transpose them
tdf <- t(df[,7:ncol(df)])

# Add "NA" values for the validation days;
# Could also drop the extra columns from from `cal`
tdf2 <- rbind(tdf, 
              matrix(NA, nrow = nrow(cal) - nrow(tdf),
                     ncol = ncol(tdf)))
tdf3 <- data.frame(cbind(cal, tdf2))

# Add in the unique names for each column
colnames(tdf3) <- c(colnames(tdf3)[1:14], df$id)
```


```{r}
# Add price information: 
# 1) Reshape the price data to have time series in rows, not columns
# 2) Since the prices are given per week, repeat them accordingly to 
#    get a full time series
# 3) For those prices which are absent at the beginning of the data,
#    we fill the missing values with the first observed price.

# 1)
prices <- subset(prices, store_id == "TX_1") %>% select(-store_id,) %>%
  spread(item_id, sell_price)
colnames(prices) <- c("wm_yr_wk", paste0("PRICE_", colnames(prices)[-1]))

# 2)
repetitions <- table(tdf3$wm_yr_wk)
prices_full <- data.frame(wm_yr_wk = rep(prices$wm_yr_wk, times = repetitions))
for (j in 2:ncol(prices))
  prices_full <- cbind(prices_full, rep(prices[,j], times = repetitions))
colnames(prices_full) <- colnames(prices)

# 3)
are_na_at_beginning <- is.na(prices_full[1,])
for (j in which(are_na_at_beginning)) {
  first_non_na <- which(!is.na(prices_full[,j]))[1]
  prices_full[1:first_non_na,j] <- prices_full[first_non_na, j]
}
```


```{r}
# Finally, combine all the data (volume + calendar + prices)
tdf4 <- merge(tdf3, prices_full, by = "wm_yr_wk")
```


```{r}
# In the following, we discard the price part and work only with the volume data.

# When using `hts`, we first need to create a multivariate time series object;
# for this we discard several calendar columns which are redundant.
TX1 <- ts(tdf3[,-(1:14)], start = c(2011, 29), end = c(2016, 116), frequency = 365)
```


```{r}
# Next we have to create a way of denoting which series is nested in another.
# This is done by supplying the `bnames` argument to hts(), and here 
# we create the corresponding names by truncating the category names to 3 letters.

# 1) Separate out the parts in the id column
splitted <- do.call(rbind, stri_split_fixed(df$id, '_'))[,1:5]
# 2) Normalise the category names
splitted[,1] <- substr(splitted[,1], 1, 3)
hts_ids <- apply(splitted, 1, paste0, collapse = '')
```


```{r}
# Create the hts object by specifying which parts of the series name correspond
# to which level of the hierarchy
TX1.h <- hts(TX1, bnames = hts_ids, characters = c(3,1,6))
```


## Forecast

```{r}
# Create training set
training_set <- window(TX1.h, start = c(2011, 29), end = c(2016, 60))
validation_set <- window(TX1.h, start = c(2016, 61), end = c(2016, 88)) 
```

#### Top-down approach

```{r}
#forecast
TX1.fc4 <- forecast(training_set, h = 28, method = "tdfp", fmethod = "rw")

# To get the accuracy measures:
acc <- accuracy.gts(TX1.fc4, validation_set)
View(acc)

plot(TX1.fc4, include = 120, levels = 0)
```


```{r}
#forecast
TX1.fc6 <- forecast(training_set, h = 28, method = "tdfp", fmethod = "arima")

# To get the accuracy measures:
acc6 <- accuracy.gts(TX1.fc6, validation_set)
View(acc)

plot(TX1.fc6, include = 120, levels = 0)
```


```{r}
#forecast
TX1.fc5 <- forecast(training_set2, h = 28, method = "tdfp", fmethod = "ets")

# To get the accuracy measures:
acc5 <- accuracy.gts(TX1.fc5, validation_set2)
View(acc5)

plot(TX1.fc5, include = 120, levels = 0)
```


#### Bottom-up approach

```{r}
#forecast
TX1.fc <- forecast(TX1.h, h = 28, method = "bu", fmethod = "rw", keep.fitted = TRUE)

# To get the accuracy measures:
acc <- accuracy.gts(TX1.fc)
View(acc)

plot(TX1.fc, include = 90, levels = 0)
```


#### Comb approach

```{r}
#forecast
TX1.fc2 <- forecast(TX1.h, h = 28, method = "comb", weights = "mint", covariance = "shr", fmethod = "rw", keep.fitted = TRUE)

# To get the accuracy measures:
acc2 <- accuracy.gts(TX1.fc2)
View(acc2)

plot(TX1.fc2, include = 90, levels = 0) 
```


```{r}
#forecast
TX1.fc3 <- forecast(TX1.h, h = 28, method = "comb", weights = "mint", covariance = "shr")

# I can't handle data with frequency greater than 24. Seasonality will be ignored. Try stlf() if you need seasonal forecasts.

# To get the accuracy measures:
acc3 <- accuracy.gts(TX1.fc3)
View(acc3)

plot(TX1.fc3, include = 90, levels = 0) 
```


## Some plots

```{r}
TX1.h %>% aggts(levels=0:1) %>%
  autoplot(facet=TRUE) 
```


```{r}
TX1.h %>% aggts(levels=1:2) %>%
  autoplot(facet=TRUE) 
```


```{r}
aggts1 <- aggts(TX1.h)
aggts2 <- aggts(TX1.h, levels = 1)
aggts3 <- aggts(TX1.h, levels = c(0, 2))
plot(TX1.h, levels = 1)
```



```{r}
TXfrequ12 <- ts(tdf3[,-(1:14)], start = c(2011, 29), end = c(2016, 116), frequency = 12)
splitted <- do.call(rbind, stri_split_fixed(df$id, '_'))[,1:5]
splitted[,1] <- substr(splitted[,1], 1, 3)
hts_ids <- apply(splitted, 1, paste0, collapse = '')
TXfrequ12.h <- hts(TXfrequ12, bnames = hts_ids, characters = c(3,1,6))

training_set2 <- window(TXfrequ12.h, start = c(2011, 29), end = c(2016, 60))
validation_set2 <- window(TXfrequ12.h, start = c(2016, 61), end = c(2016, 88)) 

plot(TXfrequ12.h, include = 120, levels = 0)

TXfrequ12.h %>% aggts(levels=1:2) %>%
  autoplot(facet=TRUE) 
```
